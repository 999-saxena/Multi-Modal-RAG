{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "jr1w02ncmqEK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Web App"
      ],
      "metadata": {
        "id": "rHzrgXFImmGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/illuin-tech/colpali.git -q\n",
        "!pip install qwen-vl-utils -q\n",
        "!pip install pinecone -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l-HhvtcF8cp",
        "outputId": "f9db4ff5-2406-44b4-edd9-27bf263e5c66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pdf2image pyngrok requests -q"
      ],
      "metadata": {
        "id": "lhhy6Iuwp4CX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install --quiet -y poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucqYMbaR7UTm",
        "outputId": "07acddda-efe8-48b5-e341-564320949ea6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb pillow pydantic -q"
      ],
      "metadata": {
        "id": "4yB439Qg7ZYC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2wOlwfSCyrR717tnSJ9QLwrzFPx_6FEiQuNMhmGtQRFnkyie1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGPXkVeQmf-4",
        "outputId": "64698f92-f479-4ccd-db04-d0adf9f62653"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile MultiModal-RAG.py\n",
        "\n",
        "from transformers.utils.import_utils import is_flash_attn_2_available\n",
        "from colpali_engine.models import BiQwen2_5, BiQwen2_5_Processor\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from PIL import Image\n",
        "\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from pdf2image import convert_from_path, convert_from_bytes\n",
        "import streamlit as st\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "import textwrap\n",
        "from matplotlib import gridspec\n",
        "import numpy as np\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"PDF -- RAG\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_name = \"nomic-ai/nomic-embed-multimodal-3b\"\n",
        "\n",
        "    model = BiQwen2_5.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"cuda:0\",\n",
        "        attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
        "    ).eval()\n",
        "\n",
        "    processor = BiQwen2_5_Processor.from_pretrained(model_name)\n",
        "\n",
        "    vlm_processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\n",
        "    vlm_model = AutoModelForVision2Seq.from_pretrained(\n",
        "        \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
        "    ).to(device)\n",
        "\n",
        "    return model, processor, vlm_model, vlm_processor\n",
        "\n",
        "\n",
        "def load_pdf():\n",
        "    PRELOADED_PDFS = {\n",
        "        \"1\": {\"title\": \"Attention Is All You Need\", \"file\": \"Attention Is All You Need.pdf\"},\n",
        "        \"2\": {\"title\": \"Deep Residual Learning\", \"file\": \"Deep Residual Learning.pdf\"}\n",
        "    }\n",
        "\n",
        "    left, middle, right = st.columns(3)\n",
        "\n",
        "    if left.button(\"Attention Is All You Need\", use_container_width=True):\n",
        "        st.session_state.choice = \"Attention Is All You Need\"\n",
        "\n",
        "    if middle.button(\"Deep Residual Learning\", use_container_width=True):\n",
        "        st.session_state.choice = \"Deep Residual Learning\"\n",
        "\n",
        "    if right.button(\"BYOP\", use_container_width=True):\n",
        "        st.session_state.choice = \"BYOP\"\n",
        "\n",
        "    choice = st.session_state.get(\"choice\", None)\n",
        "    st.write(\"You selected:\", choice)\n",
        "\n",
        "    selected = None\n",
        "    for key, pdf in PRELOADED_PDFS.items():\n",
        "        if pdf[\"title\"] == choice:\n",
        "            selected = pdf\n",
        "            break\n",
        "\n",
        "    if selected:\n",
        "        images = convert_from_path(selected[\"file\"])\n",
        "        pdf_data = {\n",
        "            \"title\": selected[\"title\"],\n",
        "            \"file\": selected[\"file\"],\n",
        "            \"images\": images,\n",
        "        }\n",
        "    elif choice == 'BYOP':\n",
        "        path_or_url = st.text_input(\"Enter local PDF path or direct PDF URL: \").strip()\n",
        "        pdf_data = None\n",
        "        if path_or_url:\n",
        "            try:\n",
        "                if path_or_url.lower().startswith(\"http\"):\n",
        "                    response = requests.get(path_or_url)\n",
        "                    response.raise_for_status()\n",
        "                    pdf_bytes = response.content\n",
        "                    title = os.path.splitext(os.path.basename(path_or_url))[0]\n",
        "                else:\n",
        "                    if not os.path.exists(path_or_url) or not path_or_url.endswith(\".pdf\"):\n",
        "                        raise ValueError(\"Invalid file path.\")\n",
        "                    with open(path_or_url, \"rb\") as f:\n",
        "                        pdf_bytes = f.read()\n",
        "                    title = os.path.splitext(os.path.basename(path_or_url))[0]\n",
        "\n",
        "                images = convert_from_bytes(pdf_bytes)\n",
        "                st.write(f\"\\nYou loaded: {title}\")\n",
        "                pdf_data = {\n",
        "                    \"title\": title,\n",
        "                    \"file\": path_or_url,\n",
        "                    \"images\": images,\n",
        "                }\n",
        "            except Exception as e:\n",
        "                st.text(f\"----- Failed to load PDF: {e} -----\")\n",
        "    else:\n",
        "        pdf_data = {\n",
        "            \"title\": None,\n",
        "            \"file\": None,\n",
        "            \"images\": None,\n",
        "        }\n",
        "\n",
        "    return pdf_data\n",
        "\n",
        "\n",
        "def display_pdf_images(images_list):\n",
        "    num_images = len(images_list)\n",
        "    num_rows = num_images // 5 + (1 if num_images % 5 > 0 else 0)\n",
        "    fig, axes = plt.subplots(num_rows, 5, figsize=(20, 4 * num_rows))\n",
        "    if num_rows == 1:\n",
        "        axes = [axes] if num_images == 1 else axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "    for i, img in enumerate(images_list):\n",
        "        if i < len(axes):\n",
        "            ax = axes[i]\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"Page {i+1}\")\n",
        "            ax.axis('off')\n",
        "    for j in range(num_images, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    plt.tight_layout()\n",
        "    st.pyplot(fig)\n",
        "\n",
        "\n",
        "def create_pine_index():\n",
        "    key = 'pcsk_2xTq6Y_SBbnbbAFXv9hfL6j7pxyvhgyV7w1iSR5h8CUG4emtaqRizX9cvp8G1o95iw6oTk'\n",
        "    pc = Pinecone(api_key=key)\n",
        "    index_name = \"rag-multimodal\"\n",
        "\n",
        "    if index_name not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=2048,\n",
        "            metric=\"cosine\",\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "\n",
        "    index = pc.Index(index_name)\n",
        "    return pc, index, index_name\n",
        "\n",
        "\n",
        "def generate_pdf_embeddings(PDFs, processor, model, index, image_save_dir=\"saved_images\"):\n",
        "    if not os.path.exists(image_save_dir):\n",
        "        os.makedirs(image_save_dir)\n",
        "\n",
        "    image_counter = 0\n",
        "    for pdf in PDFs:\n",
        "        pdf['page_embeddings'] = []\n",
        "\n",
        "        for i, image in enumerate(pdf[\"images\"]):\n",
        "            inputs = processor.process_images([image])\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                embeddings = model(**inputs)\n",
        "            embeddings = embeddings.cpu()\n",
        "            del inputs\n",
        "            embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            unique_id = f\"{pdf['title']}_page_{i+1}\"\n",
        "            image_filename = f\"{pdf['title']}_page_{i+1}.jpg\"\n",
        "            image_filepath = os.path.join(image_save_dir, image_filename)\n",
        "            image.save(image_filepath)\n",
        "\n",
        "            metadata = {\n",
        "                \"title\": pdf['title'],\n",
        "                \"page_number\": i + 1,\n",
        "                \"file\": pdf['file'],\n",
        "                \"image\": image_filepath\n",
        "            }\n",
        "\n",
        "            index.upsert(vectors=[(unique_id, embeddings[0].tolist(), metadata)])\n",
        "            image_counter += 1\n",
        "\n",
        "    print(f\"Generated embeddings for {image_counter} pages and saved images locally.\")\n",
        "\n",
        "\n",
        "def retrieve(query, index, model, processor, k=3):\n",
        "    q_inputs = processor.process_queries([query])\n",
        "    with torch.no_grad():\n",
        "        q_inputs = {k: v.to(model.device) for k, v in q_inputs.items()}\n",
        "        q_emb = model(**q_inputs).float().cpu().numpy()\n",
        "    q_emb /= np.linalg.norm(q_emb)\n",
        "\n",
        "    results = index.query(vector=[q_emb.tolist()], top_k=k, include_metadata=True)\n",
        "\n",
        "    retrieved = []\n",
        "    for match in results[\"matches\"]:\n",
        "        md = match[\"metadata\"]\n",
        "        try:\n",
        "            image = Image.open(md[\"image\"])\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ could not load image {md['image']}: {e}\")\n",
        "            image = None\n",
        "\n",
        "        retrieved.append({\n",
        "            \"title\": md[\"title\"],\n",
        "            \"file\": md[\"file\"],\n",
        "            \"page_number\": md[\"page_number\"],\n",
        "            \"score\": match[\"score\"],\n",
        "            \"image\": image\n",
        "        })\n",
        "\n",
        "    return retrieved\n",
        "\n",
        "\n",
        "def query_vlm(query, vlm_model, vlm_processor, images):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    message_content = [{\"type\": \"image\"} for _ in images]\n",
        "    message_content.append({\"type\": \"text\", \"text\": query})\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert professional PDF analyst who gives rigorous in-depth answers. \\\n",
        "            Analyse the context you've been given and give answer accordingly like an expert with relevant information.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": message_content\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    prompt = vlm_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = vlm_processor(text=prompt, images=images, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = vlm_model.generate(**inputs, max_new_tokens=1000)\n",
        "\n",
        "    response = vlm_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response\n",
        "\n",
        "\n",
        "def plot_rag_result(query, answer, images):\n",
        "    wrapped_query = '\\n'.join(textwrap.wrap(query, width=70))\n",
        "    num_images = len(images)\n",
        "    fig = plt.figure(figsize=(14, 10))\n",
        "    outer = gridspec.GridSpec(1, 2, width_ratios=[1, 1], wspace=0.1)\n",
        "\n",
        "    if num_images == 1:\n",
        "        ax1 = fig.add_subplot(outer[0])\n",
        "        ax1.imshow(images[0])\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(\"Source Document\\nretrieved by Nomic Embed Multimodal\", fontsize=12, fontweight='bold', loc='left', pad=0)\n",
        "    else:\n",
        "        left = gridspec.GridSpecFromSubplotSpec(1, 1, subplot_spec=outer[0])\n",
        "        fig.text(0.1, 0.9, \"Source Documents\\nretrieved by: Nomic Embed Multimodal\", fontsize=12, fontweight='bold', va='top', ha='left')\n",
        "        cols = min(int(np.ceil(np.sqrt(num_images))), 3)\n",
        "        rows = int(np.ceil(num_images / cols))\n",
        "        inner = gridspec.GridSpecFromSubplotSpec(rows, cols, subplot_spec=left[0], wspace=0.05, hspace=0.05)\n",
        "        for i, image in enumerate(images):\n",
        "            ax_sub = fig.add_subplot(inner[i])\n",
        "            ax_sub.imshow(image)\n",
        "            ax_sub.axis('off')\n",
        "\n",
        "    ax2 = fig.add_subplot(outer[1])\n",
        "    ax2.axis('off')\n",
        "    ax2.set_title(\"Answer generated by SMOLVLM-256M-Instruct\", fontsize=12, fontweight='bold', loc='left')\n",
        "    wrapped_answer = '\\n'.join(['\\n'.join(textwrap.wrap(line, width=80)) for line in answer.split('\\n')])\n",
        "    fontsize = min(9, max(4, 9 - ((len(wrapped_answer) - 500) // 1000)))\n",
        "    ax2.text(0.02, 0.97, wrapped_answer, transform=ax2.transAxes, fontsize=fontsize, verticalalignment='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='#2C3E50', linewidth=2, pad=1.0))\n",
        "    fig.suptitle(f\"Query: {wrapped_query}\", fontsize=14, fontweight='bold', y=0.96)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "\n",
        "def delete_index(index_name, pc):\n",
        "    pc.delete_index(index_name)\n",
        "    st.write(f\"Index {index_name} deleted.\")\n",
        "\n",
        "###################################################################################################\n",
        "# ✅ State initialization for control\n",
        "if 'choice' not in st.session_state:\n",
        "    st.session_state.choice = None\n",
        "if 'embeddings_created' not in st.session_state:\n",
        "    st.session_state.embeddings_created = False\n",
        "if 'last_choice' not in st.session_state:\n",
        "    st.session_state.last_choice = None\n",
        "if st.session_state.last_choice != st.session_state.choice:\n",
        "    st.session_state.embeddings_created = False\n",
        "    st.session_state.last_choice = st.session_state.choice\n",
        "\n",
        "model, processor, vlm_model, vlm_processor = load_models()\n",
        "st.subheader(\"Model Loaded\")\n",
        "\n",
        "pdf_data = load_pdf()\n",
        "PDFs = [pdf_data]\n",
        "\n",
        "if pdf_data and pdf_data['images']:\n",
        "    imagesss, delete_button = st.columns([1, 0.2])\n",
        "    pc, index, index_name = create_pine_index()\n",
        "\n",
        "    with imagesss:\n",
        "        with st.expander(\"First 5 Pages\"):\n",
        "            display_pdf_images(PDFs[0][\"images\"][:5])\n",
        "\n",
        "    with delete_button:\n",
        "        st.button(\"Delete Index\", on_click=delete_index, args=(index_name, pc))\n",
        "\n",
        "    # ✅ Generate embeddings once\n",
        "    if not st.session_state.embeddings_created:\n",
        "        st.write(\"Creating Embeddings\")\n",
        "        generate_pdf_embeddings(PDFs, processor, model, index)\n",
        "        st.session_state.embeddings_created = True\n",
        "\n",
        "    doc_query = st.text_input(\"Enter your query\")\n",
        "\n",
        "    if doc_query:\n",
        "        doc_rag_results = retrieve(doc_query, index, model, processor, k=1)\n",
        "        doc_image = doc_rag_results[0][\"image\"]\n",
        "        doc_answer = query_vlm(doc_query, vlm_model, vlm_processor, [doc_image])\n",
        "        plot_rag_result(doc_query, doc_answer, [doc_image])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsSwiZHrGCAe",
        "outputId": "31de8592-a93a-4844-9115-975581925cf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting MultiModal-RAG.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "!streamlit run MultiModal-RAG.py &> streamlit_log.txt &\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit app is live at:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBMRp9U7GB8_",
        "outputId": "48ec898c-9cb5-444b-d6f8-82955c0fb848"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://8a73-34-126-147-217.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debugging"
      ],
      "metadata": {
        "id": "8yOshoDhMNEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "key = 'pcsk_2xTq6Y_SBbnbbAFXv9hfL6j7pxyvhgyV7w1iSR5h8CUG4emtaqRizX9cvp8G1o95iw6oTk'\n",
        "pc = Pinecone(api_key=key)\n",
        "\n",
        "index_name = \"rag-multimodal\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "      name=index_name,\n",
        "      dimension=2048,\n",
        "      metric=\"cosine\",\n",
        "      spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "      )\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "vYWgWrL1GB6B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjdaDq-p5aRt",
        "outputId": "66a60eb8-fbf3-4d13-dc4a-cdbe0b3dbb54"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 2048,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'': {'vector_count': 12}},\n",
              " 'total_vector_count': 12,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twJHZgdk_1j6",
        "outputId": "31f0c6cf-f24e-4b68-aed1-3865cc25c9f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\n",
              "    {\n",
              "        \"name\": \"rag-multimodal\",\n",
              "        \"metric\": \"cosine\",\n",
              "        \"host\": \"rag-multimodal-ejzuolb.svc.aped-4627-b74a.pinecone.io\",\n",
              "        \"spec\": {\n",
              "            \"serverless\": {\n",
              "                \"cloud\": \"aws\",\n",
              "                \"region\": \"us-east-1\"\n",
              "            }\n",
              "        },\n",
              "        \"status\": {\n",
              "            \"ready\": true,\n",
              "            \"state\": \"Ready\"\n",
              "        },\n",
              "        \"vector_type\": \"dense\",\n",
              "        \"dimension\": 2048,\n",
              "        \"deletion_protection\": \"disabled\",\n",
              "        \"tags\": null\n",
              "    }\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"rag-multimodal\"\n",
        "pc.delete_index(index_name)\n",
        "pc.list_indexes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZqGSKySGB3U",
        "outputId": "02227498-e6c8-4e13-fffc-9699f75d9666"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2go90GrGB0c"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}